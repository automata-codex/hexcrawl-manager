import yaml
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
from markdownify import markdownify as md_to_text

# Paths
CLUES_DIR = Path("../../data/floating-clues")
HEXES_DIR = Path("../../data/hexes")
OUTPUT_FILE = Path("../../data/clue-links.yaml")

# Settings
TOP_N_MATCHES = 3

# Load model
print("ðŸ”® Loading sentence transformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load clues
def load_clues():
    clues = []
    clue_files = list(CLUES_DIR.glob("**/*.yml")) + list(CLUES_DIR.glob("**/*.yaml"))
    for file in clue_files:
        with open(file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            base_text = f"{data['name']}: {data['summary']}"
            detail_text = md_to_text(data.get("detailText", "")).strip()
            combined_text = f"{base_text}\n{detail_text}".strip()
            clues.append({
                "id": data["id"],
                "text": combined_text
            })
    return clues

# Load hexes
def load_hexes():
    hexes = []
    hex_files = list(HEXES_DIR.glob("**/*.yml")) + list(HEXES_DIR.glob("**/*.yaml"))
    for file in hex_files:
        with open(file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            text_parts = []

            if "landmark" in data:
                text_parts.append(data["landmark"])

            if "hiddenSites" in data:
                for hs in data["hiddenSites"]:
                    if isinstance(hs, str):
                        text_parts.append(hs)
                    elif isinstance(hs, dict):
                        text_parts.append(hs.get("description", ""))

            if "notes" in data:
                for note in data["notes"]:
                    text_parts.append(note)

            flat_text_parts = []
            for part in text_parts:
                if isinstance(part, list):
                    for subpart in part:
                        flat_text_parts.append(str(subpart))
                else:
                    flat_text_parts.append(str(part))

            text = "\n".join(flat_text_parts)

            hexes.append({
                "id": data["slug"],
                "text": text
            })

    return hexes

# Main logic
def main():
    print("ðŸ“š Loading clues and hexes...")
    clues = load_clues()
    hexes = load_hexes()

    print(f"ðŸ§  Encoding {len(clues)} clues and {len(hexes)} hexes...")
    clue_embeddings = model.encode([c["text"] for c in clues])
    hex_embeddings = model.encode([h["text"] for h in hexes])

    print("ðŸ“ˆ Computing cosine similarity...")
    similarity_matrix = cosine_similarity(clue_embeddings, hex_embeddings)

    print("ðŸ”— Selecting top matches...")
    output = []
    for i, clue in enumerate(clues):
        similarity_scores = similarity_matrix[i]
        top_indices = np.argsort(similarity_scores)[::-1][:TOP_N_MATCHES]
        linked = []
        for j in top_indices:
            linked.append({
                "hexId": hexes[j]["id"],
                "score": float(f"{similarity_scores[j]:.4f}")  # Round for readability
            })
        output.append({
            "clueId": clue["id"],
            "linkedHexes": linked
        })

    print(f"ðŸ’¾ Writing output to {OUTPUT_FILE}...")
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        # Write timestamp comment
        now = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
        f.write(f"# This file was autogenerated by generate_clue_links.py on {now}\n\n")
        yaml.dump(output, f, sort_keys=False)

    print("âœ… Done!")

if __name__ == "__main__":
    main()
