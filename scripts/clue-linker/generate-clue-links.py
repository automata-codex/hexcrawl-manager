import yaml
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
from markdownify import markdownify as md_to_text
import re

# Paths (relative to the project root, not this file, because it gets called by npm in the project root)
CLUES_DIR = Path("./data/floating-clues")
HEXES_DIR = Path("./data/hexes")
OUTPUT_FILE = Path("./data/clue-links.yaml")

# Settings
MATCH_THRESHOLD = 0.4
TAG_BOOST = 0.05  # How much to boost similarity per matching tag
TOP_N_MATCHES = 5

# Load model
print("ðŸ”® Loading sentence transformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2')

# Helper: Load YAML files
def load_yaml_files(directory):
    files = list(directory.glob("**/*.yml")) + list(directory.glob("**/*.yaml"))
    return [yaml.safe_load(file.read_text(encoding="utf-8")) for file in files]

# Load clues
def load_clues():
    clues = []
    for data in load_yaml_files(CLUES_DIR):
        name = data["name"]
        summary = data["summary"]
        base_text = f"{name}: {summary}"
        detail_text = md_to_text(data.get("detailText", "")).strip()
        combined_text = f"{base_text}\n{detail_text}".strip()
        clues.append({
            "id": data["id"],
            "name": name,
            "summary": summary,
            "text": combined_text,
            "tags": data.get("tags", [])
        })
    return clues

# Load hexes
def load_hexes():
    hexes = []
    for data in load_yaml_files(HEXES_DIR):
        text_parts = []

        if "landmark" in data:
            text_parts.append(data["landmark"])

        if "hiddenSites" in data:
            for hs in data["hiddenSites"]:
                if isinstance(hs, str):
                    text_parts.append(hs)
                elif isinstance(hs, dict):
                    text_parts.append(hs.get("description", ""))

        if "notes" in data:
            for note in data["notes"]:
                text_parts.append(note)

        flat_text_parts = []
        for part in text_parts:
            if isinstance(part, list):
                for subpart in part:
                    flat_text_parts.append(str(subpart))
            else:
                flat_text_parts.append(str(part))

        hexes.append({
            "id": data["slug"],
            "text": "\n".join(flat_text_parts),
            "tags": data.get("tags", [])
        })

    return hexes

# Main logic
def main():
    print("ðŸ“š Loading clues and hexes...")
    clues = load_clues()
    hexes = load_hexes()

    print(f"ðŸ§  Encoding {len(clues)} clues and {len(hexes)} hexes...")
    clue_embeddings = model.encode([c["text"] for c in clues])
    hex_embeddings = model.encode([h["text"] for h in hexes])

    print("ðŸ”— Matching clues to hexes...")
    output = []

    for clue_idx, clue in enumerate(clues):
        clue_tags = set(tag.lower() for tag in clue.get("tags", []))

        linked = []

        # Handle forced hex placements first
        forced_hex_tags = [tag for tag in clue_tags if tag.startswith("hex-")]
        forced_hex_ids = [tag.replace("hex-", "") for tag in forced_hex_tags]

        for hex_id in forced_hex_ids:
            linked.append({
                "hexId": hex_id,
                "score": 1.0  # Always perfect match
            })

        # Now do fuzzy matching
        similarity_scores = cosine_similarity(
            [clue_embeddings[clue_idx]],
            hex_embeddings
        )[0]

        for hex_idx, hex_ in enumerate(hexes):
            base_score = similarity_scores[hex_idx]
            hex_tags = set(tag.lower() for tag in hex_.get("tags", []))

            # Boost if tags overlap
            common_tags = clue_tags & hex_tags
            boosted_score = base_score + (len(common_tags) * TAG_BOOST)

            if boosted_score >= MATCH_THRESHOLD:
                hex_id = hex_["id"]
                # Don't add a fuzzy link if it's already a forced link
                if hex_id not in forced_hex_ids:
                    linked.append({
                        "hexId": hex_id,
                        "score": float(f"{boosted_score:.4f}")
                    })

        # Sort all linked hexes by descending score
        linked.sort(key=lambda x: x["score"], reverse=True)

        # Limit to top N matches
        linked = linked[:TOP_N_MATCHES]

        if linked:
            output.append({
                "clueId": clue["id"],
                "name": clue["name"],
                "summary": clue["summary"],
                "linkedHexes": linked
            })

    print(f"ðŸ’¾ Writing output to {OUTPUT_FILE}...")
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        # Write timestamp comment
        now = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
        f.write(f"# This file was autogenerated by generate_clue_links.py on {now}\n\n")
        yaml.dump(output, f, sort_keys=False)

    print("âœ… Done!")

if __name__ == "__main__":
    main()
